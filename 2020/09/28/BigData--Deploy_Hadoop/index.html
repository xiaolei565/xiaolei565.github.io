<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>大数据--实分布部署Hadoop | xiaolei565</title><meta name="keywords" content="hadoop,分布式,大数据"><meta name="author" content="Glor"><meta name="copyright" content="Glor"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="description" content="大数据–实分布部署Hadoop系统及环境网络环境：处于局域网下的部分机器（&gt;&#x3D;4）系统版本：win10较新版本，支持子系统子系统版本：ubuntu18.04其他软件：Xshell6，Xftp6实验要求：在多台机器上安装hadoop，配置完成后，可以在完成官方demo的wordcount功能，实现其他功能">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据--实分布部署Hadoop">
<meta property="og:url" content="https://xiaolei565.github.io/2020/09/28/BigData--Deploy_Hadoop/index.html">
<meta property="og:site_name" content="xiaolei565">
<meta property="og:description" content="大数据–实分布部署Hadoop系统及环境网络环境：处于局域网下的部分机器（&gt;&#x3D;4）系统版本：win10较新版本，支持子系统子系统版本：ubuntu18.04其他软件：Xshell6，Xftp6实验要求：在多台机器上安装hadoop，配置完成后，可以在完成官方demo的wordcount功能，实现其他功能">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://xiaolei565.github.io/null">
<meta property="article:published_time" content="2020-09-28T12:48:05.936Z">
<meta property="article:modified_time" content="2020-05-11T13:40:50.000Z">
<meta property="article:author" content="Glor">
<meta property="article:tag" content="hadoop">
<meta property="article:tag" content="分布式">
<meta property="article:tag" content="大数据">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://xiaolei565.github.io/null"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://xiaolei565.github.io/2020/09/28/BigData--Deploy_Hadoop/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.2.0',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  ClickShowText: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
};

var saveToLocal = {
  set: function setWithExpiry(key, value, ttl) {
    const now = new Date()
    const expiryDay = ttl * 86400000
    const item = {
      value: value,
      expiry: now.getTime() + expiryDay,
    }
    localStorage.setItem(key, JSON.stringify(item))
  },

  get: function getWithExpiry(key) {
    const itemStr = localStorage.getItem(key)

    if (!itemStr) {
      return undefined
    }
    const item = JSON.parse(itemStr)
    const now = new Date()

    if (now.getTime() > item.expiry) {
      localStorage.removeItem(key)
      return undefined
    }
    return item.value
  }
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2020-05-11 21:40:50'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
  }
}

var autoChangeMode = 'false'
var t = saveToLocal.get('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (saveToLocal.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><meta name="generator" content="Hexo 5.2.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/null" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">314</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">130</div></a></div></div></div><hr/></div></div><div id="body-wrap"><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E2%80%93%E5%AE%9E%E5%88%86%E5%B8%83%E9%83%A8%E7%BD%B2Hadoop"><span class="toc-number">1.</span> <span class="toc-text">大数据–实分布部署Hadoop</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F%E5%8F%8A%E7%8E%AF%E5%A2%83"><span class="toc-number">1.1.</span> <span class="toc-text">系统及环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85wsl%EF%BC%88ubuntu%EF%BC%89"><span class="toc-number">1.2.</span> <span class="toc-text">下载安装wsl（ubuntu）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85ssh%E5%B9%B6%E8%AE%BE%E7%BD%AE%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95"><span class="toc-number">1.3.</span> <span class="toc-text">安装ssh并设置免密码登录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Java%E5%B9%B6%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-number">1.4.</span> <span class="toc-text">安装Java并配置环境变量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E8%8A%82%E7%82%B9%E9%85%8D%E7%BD%AE%EF%BC%88%E9%87%8D%E8%A6%81%EF%BC%89"><span class="toc-number">1.5.</span> <span class="toc-text">其他节点配置（重要）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%8A%82%E7%82%B9%E5%AE%89%E8%A3%85hadoop%E5%B9%B6%E5%88%86%E5%8F%91%E5%88%B0%E5%85%B6%E4%BB%96%E8%8A%82%E7%82%B9"><span class="toc-number">1.6.</span> <span class="toc-text">主节点安装hadoop并分发到其他节点</span></a></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">xiaolei565</a></span><span id="menus"><span class="close" id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">大数据--实分布部署Hadoop</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-09-28T12:48:05.936Z" title="发表于 2020-09-28 20:48:05">2020-09-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2020-05-11T13:40:50.000Z" title="更新于 2020-05-11 21:40:50">2020-05-11</time></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h2 id="大数据–实分布部署Hadoop"><a href="#大数据–实分布部署Hadoop" class="headerlink" title="大数据–实分布部署Hadoop"></a>大数据–实分布部署Hadoop</h2><h3 id="系统及环境"><a href="#系统及环境" class="headerlink" title="系统及环境"></a>系统及环境</h3><p>网络环境：处于局域网下的部分机器（&gt;=4）<br>系统版本：win10较新版本，支持子系统<br>子系统版本：ubuntu18.04<br>其他软件：Xshell6，Xftp6<br>实验要求：在多台机器上安装hadoop，配置完成后，可以在完成官方demo的wordcount功能，实现其他功能</p>
<a id="more"></a>

<h3 id="下载安装wsl（ubuntu）"><a href="#下载安装wsl（ubuntu）" class="headerlink" title="下载安装wsl（ubuntu）"></a>下载安装wsl（ubuntu）</h3><ol>
<li>系统要求：win10较新版本</li>
<li>修改电脑名（用以在ubuntu中分辨是哪台电脑,这一步推荐在后面其他节点修改配置里做）：右击此电脑-属性-更改设置-计算机描述（ustc+序号）-更改-计算机名（ustc+序号）-稍后重启</li>
<li>关闭防火墙：控制面板-网络和Internet-网络和共享中心-wd防火墙（左下角）-启用或关闭wd防火墙-稍后重启</li>
<li>开启wsl功能：打开控制面板-程序-启用或关闭windows功能-勾选适用于Linux的Windows子系统-重启电脑</li>
<li>打开Microsoft Store，搜索ubuntu，选择18.04下载</li>
<li>先创建ustc用户，密码也为ustc（只是为了好记）</li>
<li>创建hadoop用户</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$sudo useradd -m hadoop -s &#x2F;bin&#x2F;bash # 创建hadoop用户，并使用&#x2F;bin&#x2F;shell作为shell</span><br><span class="line">$sudo passwd hadoop #为hadoop用户设置密码</span><br><span class="line">$sudo adduser hadoop sudo #为hadoop用户增加管理员权限</span><br><span class="line">$su - hadoop #切换当前用户为hadoop</span><br><span class="line">$sudo apt-get update #更新hadoop用户的apt，方便后面安装ssh</span><br></pre></td></tr></table></figure>

<h3 id="安装ssh并设置免密码登录"><a href="#安装ssh并设置免密码登录" class="headerlink" title="安装ssh并设置免密码登录"></a>安装ssh并设置免密码登录</h3><ol>
<li>安装ssh,生成公钥和私钥</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install openssh-server	#安装SSH server</span><br><span class="line">$ cd &#x2F;home&#x2F;hadoop # 进入hadoop下</span><br><span class="line">$ ssh-keygen -t rsa #一直回车，会在用户文件夹下生成.ssh文件夹，文件夹下生成三个文件，不要添加sudo，否在会报错</span><br><span class="line">$ sudo &#x2F;etc&#x2F;init.d&#x2F;ssh start #启动ssh服务  </span><br><span class="line">若启动报错，则将没有的文件对应生成一下</span><br><span class="line">	$ sudo ssh-keygen -t rsa -f &#x2F;etc&#x2F;xxx</span><br><span class="line">$ ps -e |grep ssh	#查看ssh进程,显示有ssh服务则为成功</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>生成密钥和公钥并将公钥复制给其他机器</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">免密登录过程简介：本机生成公钥id_rsa.pub(上一步已经做好)，将生成的公钥上传到目标机器上的授权文件authorized_keys，本机即可免密登录目标机</span><br><span class="line"></span><br><span class="line">创建授权文件：将id_rsa.pub的内容给authorized_keys,若没有auth文件则会自动生成一个</span><br><span class="line">$ cd &#x2F;home&#x2F;hadoop&#x2F;.ssh #进入用户文件夹下面的.ssh文件夹</span><br><span class="line">$ cat id_rsa.pub&gt;&gt;authorized_keys</span><br><span class="line"></span><br><span class="line">给.ssh文件夹和authorized_keys授权，权限如下</span><br><span class="line">$cd &#x2F;home&#x2F;hadoop&#x2F;.ssh  #已经在这一步请不用操作</span><br><span class="line">$sudo chmod 600 authorized_keys</span><br><span class="line">$cd ..#回到上一层目录</span><br><span class="line">$sudo chmod 700 .ssh</span><br><span class="line">$cd ..#回到上一层目录</span><br><span class="line">$sudo chmod 700 hadoop #将.ssh上级目录授权为700</span><br><span class="line"></span><br><span class="line">修改配置</span><br><span class="line">$sudo vim &#x2F;etc&#x2F;ssh&#x2F;sshd_config</span><br><span class="line">	Port xxx   (可以修改成其他统一端口号)</span><br><span class="line">	PermitRootLogin yes</span><br><span class="line">	将下面配置更改为yes</span><br><span class="line">	PasswordAuthentication yes(不修改会报权限问题)</span><br><span class="line">$ sudo &#x2F;etc&#x2F;init.d&#x2F;ssh restart #重启ssh服务 </span><br><span class="line"></span><br><span class="line">将客户端的公钥~&#x2F;.ssh&#x2F;id_rsa.pub通过ssh-copy-id -i拷贝到服务器（只需要将master节点的公钥复制给其他datanode节点，其他的可以不用复制）</span><br><span class="line">$ ssh-copy-id -i ~&#x2F;.ssh&#x2F;id_rsa.pub user@xxx.xxx.xxx.xxx -p xxxx</span><br><span class="line">	user代表Linux用户，xxx.xxx.xxx.xxx代表远程主机地址，下面为例子：</span><br><span class="line">	$ ssh-copy-id -i ~&#x2F;.ssh&#x2F;id_rsa.pub root@192.168.1.xxx -p xxxx</span><br><span class="line">需要输入目标机器用户的密码，如hadoop用户的密码hadoop</span><br><span class="line"></span><br><span class="line">如果公钥拷贝没有报错，使用命令即可免密码登录另一台机器</span><br><span class="line">$ ssh -p 6666 hadoop@192.168.1.xxx	#-p后面代表的是端口号 就是上一步修改的配置里面的Port</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">以上免密为本机登录其他机器的操作，并不需要每台机器都需要进行拷贝操作，只需要一台作为主机分发到其他机器即可</span><br></pre></td></tr></table></figure>

<h3 id="安装Java并配置环境变量"><a href="#安装Java并配置环境变量" class="headerlink" title="安装Java并配置环境变量"></a>安装Java并配置环境变量</h3><ol>
<li>安装xshell和xftp,安装直接一路确认即可</li>
<li>尝试使用xshell连接自己的机子，以下命令都可以在xshell里运行，当然原来的界面也是可以的（连接步骤略）</li>
<li>安装java，并配置环境</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkdir &#x2F;usr&#x2F;local&#x2F;java#在&#x2F;usr&#x2F;local下创建java文件夹</span><br><span class="line"></span><br><span class="line">将jdk-8xxxx-linux-x64复制到c盘下，右击属性给文件授予全部权限，以便方便上传到wsl里面</span><br><span class="line">$ cd &#x2F;mnt&#x2F;c #进入win的c盘</span><br><span class="line">$ cp jdk-8xxxx-linux-x64.tar.gz &#x2F;usr&#x2F;local&#x2F;java</span><br><span class="line"></span><br><span class="line">进入java文件夹，并进行解压</span><br><span class="line">$cd java&#x2F;</span><br><span class="line">$tar -zxvf jdk-8xxxx-linux-x64.tar.gz#解压</span><br><span class="line">java文件下面会出现jdk1.8xxxx文件夹</span><br><span class="line"></span><br><span class="line">如果遇到权限问题或者进不去java文件夹，请将&#x2F;usr&#x2F;local的文件夹授予权限</span><br><span class="line">	$ sudo chmod 777 &#x2F;usr&#x2F;local&#x2F;java</span><br><span class="line"></span><br><span class="line">配置环境变量</span><br><span class="line">$ sudo vim &#x2F;etc&#x2F;profile</span><br><span class="line">    export JAVA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;java&#x2F;jdk1.8xxxx</span><br><span class="line">    export PATH&#x3D;$&#123;PATH&#125;:$&#123;JAVA_HOME&#125;&#x2F;bin</span><br><span class="line">注意：“&#x3D;”两边不要有空格</span><br><span class="line"></span><br><span class="line">使环境变量生效</span><br><span class="line">$ source &#x2F;etc&#x2F;profile</span><br><span class="line">注意：请不要加sudo，source是bash的内建命令，那样会报找不到命令的错误</span><br><span class="line"></span><br><span class="line">验证java环境是否配置完成</span><br><span class="line">$ java -version</span><br><span class="line"></span><br><span class="line">如果正常命令下来还是找不到，请参考：</span><br><span class="line">	告诉Ubuntu  Java JDK的位置</span><br><span class="line">    $ sudo update-alternatives --install &quot;&#x2F;usr&#x2F;bin&#x2F;java&quot; &quot;java&quot; &quot;&#x2F;usr&#x2F;local&#x2F;java&#x2F;jdk1.8.0_231&#x2F;bin&#x2F;java&quot; 1</span><br><span class="line">    $ sudo update-alternatives --install &quot;&#x2F;usr&#x2F;bin&#x2F;javac&quot; &quot;javac&quot; &quot;&#x2F;usr&#x2F;local&#x2F;java&#x2F;jdk1.8.0_231&#x2F;bin&#x2F;javac&quot; 1</span><br><span class="line">    $ sudo update-alternatives --install &quot;&#x2F;usr&#x2F;bin&#x2F;javaws&quot; &quot;javaws&quot; &quot;&#x2F;usr&#x2F;local&#x2F;java&#x2F;jdk1.8.0_231&#x2F;bin&#x2F;javaws&quot; 1</span><br><span class="line">    相应的将其设为默认方式：</span><br><span class="line">    $ sudo update-alternatives --set java  &#x2F;usr&#x2F;local&#x2F;java&#x2F;jdk1.8.0_231&#x2F;bin&#x2F;java</span><br><span class="line">    $ sudo update-alternatives --set javac  &#x2F;usr&#x2F;local&#x2F;java&#x2F;jdk1.8.0_231&#x2F;bin&#x2F;javac</span><br><span class="line">    $ sudo update-alternatives --set javaws &#x2F;usr&#x2F;local&#x2F;java&#x2F;jdk1.8.0_231&#x2F;bin&#x2F;javaws</span><br><span class="line">参考文章：https:&#x2F;&#x2F;blog.csdn.net&#x2F;weixin_42001089&#x2F;article&#x2F;details&#x2F;81865101</span><br></pre></td></tr></table></figure>

<h3 id="其他节点配置（重要）"><a href="#其他节点配置（重要）" class="headerlink" title="其他节点配置（重要）"></a>其他节点配置（重要）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">将所有的主机的host都配置一下，添加每台机器的ip和对应名称，如</span><br><span class="line">这里由于wsl和win系统用的同一个hosts，所以我们需要修改win10下面的hosts</span><br><span class="line"></span><br><span class="line">我们打开此电脑-C盘-window-system32-divers-etc-hosts</span><br><span class="line"></span><br><span class="line">将所有配置项全部注释，在前面加上#</span><br><span class="line">添加以下若干项，这里以我们配置的四台机器为例</span><br><span class="line">	192.168.1.100 ustc01</span><br><span class="line">	192.168.1.158 ustc02</span><br><span class="line">	192.168.1.58  ustc03</span><br><span class="line">	192.168.1.75  ustc04</span><br><span class="line">	（重要）将其他ip配置关闭，前面全部添加#，只保留上面四个配置，要不然在之后hadoop的监控页面看不到datanode节点信息。 </span><br><span class="line">	</span><br><span class="line">如果因为没有权限进行保存，可以先拷贝到桌面进行修改，修改完复制回去覆盖</span><br><span class="line"></span><br><span class="line">重启win电脑</span><br><span class="line">记得开电脑后，开启一下ssh服务，命令见最后面的演示段落</span><br><span class="line"></span><br><span class="line">如果Java是单独每个人配置的，以下步骤可以不做</span><br><span class="line">分发hadoop到其他节点</span><br><span class="line">$scp -P 6666 &#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.7.tar.gz ustc02:&#x2F;home&#x2F;hadoop #-P为大写，后接端口号。</span><br><span class="line">$scp -P 6666 &#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.7.tar.gz ustc03:&#x2F;home&#x2F;hadoop #-P为大写。</span><br><span class="line">$scp -P 6666 &#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.7.tar.gz ustc04:&#x2F;home&#x2F;hadoop #-P为大写。	</span><br><span class="line"></span><br><span class="line">解压与配置和前面的java是一样的，参考前面，写了的话请忽略</span><br></pre></td></tr></table></figure>



<h3 id="主节点安装hadoop并分发到其他节点"><a href="#主节点安装hadoop并分发到其他节点" class="headerlink" title="主节点安装hadoop并分发到其他节点"></a>主节点安装hadoop并分发到其他节点</h3><p>（本配置只需要主要的namenode进行配置即可，其他datanode之后进行分发即可）</p>
<ol>
<li><p>类似java的上传，将hadoop压缩包放到c盘下进行授权，并上传，本次上传路径为/home/hadoop(命令省略，请参考java配置)</p>
</li>
<li><p>配置hadoop的四个xml文件和一个子节点标识文件（所有节点这几个信息请保持一致）</p>
<ol>
<li><p>修改core-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;fs.default.name&lt;&#x2F;name&gt;</span><br><span class="line">		&lt;value&gt;hdfs:&#x2F;&#x2F;ustc01:9000&lt;&#x2F;value&gt;</span><br><span class="line">	&lt;&#x2F;property&gt;		</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改hdfs-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">		&lt;value&gt;3&lt;&#x2F;value&gt;</span><br><span class="line">		&lt;description&gt;副本个数，配置默认是3,应小于datanode机器数量&lt;&#x2F;description&gt;</span><br><span class="line">	&lt;&#x2F;property&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.namenode.dir&lt;&#x2F;name&gt;</span><br><span class="line">		&lt;value&gt;file:&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.7&#x2F;tmp&#x2F;dfs&#x2F;name&lt;&#x2F;value&gt;</span><br><span class="line">		&lt;description&gt;namenode上数据块的物理存储位置&lt;&#x2F;description&gt;</span><br><span class="line">	&lt;&#x2F;property&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;dfs.datanode.dir&lt;&#x2F;name&gt;</span><br><span class="line">		&lt;value&gt;file:&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.7&#x2F;tmp&#x2F;dfs&#x2F;data&lt;&#x2F;value&gt;</span><br><span class="line">		&lt;description&gt;datanode上数据块的物理存储位置&lt;&#x2F;description&gt;</span><br><span class="line">	&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		 &lt;name&gt;dfs.name.data.registration.ip-hostname-check&lt;&#x2F;name&gt;</span><br><span class="line">		 &lt;value&gt;false&lt;&#x2F;value&gt;</span><br><span class="line">	&lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改mapred-site.xml，本文件没有，需要运行以下命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv mapred-site.xml.template mapred-site.xml  #重命名</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#这里是配置信息</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt;</span><br><span class="line">		&lt;value&gt;yarn&lt;&#x2F;value&gt;</span><br><span class="line">	 &lt;&#x2F;property&gt;</span><br><span class="line">	 &lt;!--&lt;property&gt;</span><br><span class="line">	  	&lt;name&gt;mapred.job.tracker&lt;&#x2F;name&gt;</span><br><span class="line">		&lt;value&gt;hdfs:&#x2F;&#x2F;ustc01:9001&lt;&#x2F;value&gt;</span><br><span class="line">	 &lt;&#x2F;property&gt;--&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改yarn-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- Site specific YARN configuration properties --&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt;</span><br><span class="line">		&lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;</span><br><span class="line">	&lt;&#x2F;property&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt;</span><br><span class="line">		&lt;value&gt;ustc01&lt;&#x2F;value&gt;</span><br><span class="line">		&lt;description&gt;resourcemanager位置&lt;&#x2F;description&gt;</span><br><span class="line">	&lt;&#x2F;property&gt;</span><br><span class="line">	&lt;property&gt;</span><br><span class="line">		&lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;&#x2F;name&gt;</span><br><span class="line">		&lt;value&gt;1000&lt;&#x2F;value&gt;</span><br><span class="line">		&lt;description&gt;虚拟内存是物理内存的多少倍，这里请根据自己机器配置，默认是2.1，我这里改成1000是因为，运行wordcount时虚拟内存需要1.2TB，但是物理内存只有2G，所以我直接写了1000，这里很重要就是这个导致我们一致测试出错，这里就是原因所在，&lt;&#x2F;description&gt;</span><br><span class="line">		</span><br><span class="line">	&lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br><span class="line"></span><br><span class="line">比如卡在job,或者卡在map0%，reduce 0%,请把vmem-check-enabled的false关掉，或者不要多配，这里没看懂的，请忽略，遇到类似问题再看这个</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改slaves文件</p>
<p>这里存放datanode节点名,主机节点名称不要添加，如</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ustc02</span><br><span class="line">ustc03</span><br><span class="line">ustc04</span><br><span class="line"></span><br><span class="line">请输入你需要连接的所有datanode节点名</span><br></pre></td></tr></table></figure>
</li>
</ol>
</li>
<li><p>分发到其他节点，并配置</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">将配置好的文件夹，压缩打包</span><br><span class="line">$ cd &#x2F;home&#x2F;hadoop</span><br><span class="line">$ rm hadoop-2.7.7.tat.gz  #删除原来上传的压缩包</span><br><span class="line">$ tar -zcf  hadoop-2.7.7.tar.gz	hadoop-2.7.7&#x2F;</span><br><span class="line"></span><br><span class="line">$ scp -P 6666 hadoop-2.7.7.tar.gz ustc02:&#x2F;home&#x2F;hadoop #分发hadoop，-P为大写。</span><br><span class="line">这里解压也是类似java解压配置配置</span><br><span class="line"></span><br><span class="line">配置环境变量</span><br><span class="line">$ sudo vim &#x2F;etc&#x2F;profile   #在刚刚配置的java环境下添加配置成如下格式</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;java&#x2F;jdk1.8.0_231</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.7 exportPATH&#x3D;$&#123;PATH&#125;:$&#123;JAVA_HOME&#125;&#x2F;bin:$&#123;HADOOP_HOME&#125;&#x2F;bin:$&#123;HADOOP_HOME&#125;&#x2F;sbin:</span><br><span class="line">使环境变量生效</span><br><span class="line">$ source &#x2F;etc&#x2F;profile</span><br><span class="line">注意：请不要加sudo，source是bash的内建命令，那样会报找不到命令的错误</span><br><span class="line"></span><br><span class="line">验证hadoop环境是否配置完成</span><br><span class="line">$ hadoop</span><br><span class="line">如果没有报错就是成功配置了环境</span><br><span class="line"></span><br><span class="line">由于之前我们修改了默认的22端口为6666，这里修改Hadoop的相关端口号</span><br><span class="line">在hadoop-2.7.7&#x2F;etc&#x2F;hadoop目录修改 hadoop-env.sh 文件添加如下语句。</span><br><span class="line">在最后面添加，方便寻找</span><br><span class="line">$sudo vim &#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.7&#x2F;etc&#x2F;hadoop&#x2F;hadoop-env.sh</span><br><span class="line">	export HADOOP_SSH_OPTS&#x3D;&quot;-p 6666&quot;</span><br><span class="line">	将JAVA_HOME等号后面用&#x2F;usr&#x2F;local&#x2F;java&#x2F;jdk1.8.0_231替换掉</span><br><span class="line">	JAVA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;java&#x2F;jdk1.8.0_231</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>开启与关闭以及验证</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop namenode -format #格式化主节点，本条命令请不要乱输入，请部署完所有节点之后进行输入</span><br><span class="line"></span><br><span class="line">#开启与关闭，只需要在namenode节点上输入,不要都输入了</span><br><span class="line">$ start-all.sh #开启全部服务</span><br><span class="line">$ stop-all.sh #关闭全部服务</span><br><span class="line">$ start-dfs.sh  #开启hdfs</span><br><span class="line">$ start-dfs.sh #关闭hdfs</span><br><span class="line"></span><br><span class="line">验证是否已经启动，在每个节点输入</span><br><span class="line">$ jps</span><br><span class="line">如果在namenode节点上显示了namenode，在datenode上显示了datanode就表示启动正常</span><br><span class="line"></span><br><span class="line">打开win系统的浏览器，输入http:&#x2F;&#x2F;ustc01:50070,如果显示namenode与datanode节点信息正确即成功</span><br><span class="line">这个网址也是心跳监管系统，展示时需要打开</span><br></pre></td></tr></table></figure>

<ol start="5">
<li><p><strong>代码（@TODO 冉哥豪哥 done）</strong></p>
</li>
<li><p><strong>文档（@TODO锋哥辉哥雷哥林哥ing）</strong></p>
</li>
<li><p><strong>剩余部署（@TODO洋哥ing）</strong></p>
<p>​                                                                                                                                                        2019-11-20</p>
</li>
</ol>
<p>开机演示需要进行的操作：</p>
<ol>
<li><p>打开wsl</p>
</li>
<li><p>运行ssh（可以设置自启动,这里懒得设置）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">演示时需要每台机子运行这条命令，开启ssh命令</span><br><span class="line">$sudo service ssh --full-restart</span><br></pre></td></tr></table></figure>
</li>
<li><p>一开机，hosts就变回去了，这个还没解决（待修复）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$sudo vim &#x2F;etc&#x2F;hosts</span><br><span class="line">	192.168.1.100 ustc01</span><br><span class="line">	192.168.1.158 ustc02</span><br><span class="line">	192.168.1.58  ustc03</span><br><span class="line">	192.168.1.75  ustc04</span><br></pre></td></tr></table></figure>





</li>
</ol>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Glor</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://xiaolei565.github.io/2020/09/28/BigData--Deploy_Hadoop/">https://xiaolei565.github.io/2020/09/28/BigData--Deploy_Hadoop/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://xiaolei565.github.io" target="_blank">xiaolei565</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/hadoop/">hadoop</a><a class="post-meta__tags" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/">分布式</a><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></div><div class="post_share"><div class="social-share" data-image="/null" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/09/28/BigData--Deploy_Hadoop2/"><img class="prev-cover" src="/null" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">大数据--创建wordcount</div></div></a></div><div class="next-post pull-right"><a href="/2020/09/28/Algorithm--Recursion/"><img class="next-cover" src="/null" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">三道题套路解决递归问题</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/09/28/BigData--Deploy_Hadoop2/" title="大数据--创建wordcount"><img class="cover" src="/null"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-09-28</div><div class="title">大数据--创建wordcount</div></div></a></div></div></div></article></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Glor</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><div class="js-pjax"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></div></body></html>